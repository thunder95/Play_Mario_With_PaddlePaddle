{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 飞桨黑客马拉松比赛介绍\n",
    "\n",
    "2021 PaddlePaddle Hackathon 飞桨黑客马拉松，是由飞桨联合深度学习技术及应用国家工程实验室主办，联合 OpenVINO、MLFlow、KubeFlow、TVM 等开源项目共同出品，面向全球开发者的深度学习领域编程活动，旨在鼓励开发者了解与参与深度学习开源项目。\n",
    "\n",
    "<div align = \"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/631c300925fd4427a7e9a588a83e9487781a813044b44e18b023d870a1357631\" alt=\"图片替换文本\" width=\"300\" height=\"500\" align=\"center\" />\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/589a249ca4da453ba95011ae2be228905859739ff0964751a69f7ba54bdd346f\" alt=\"图片替换文本\" width=\"700\" height=\"600\" align=\"center\" />\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 参赛项目介绍\n",
    "\n",
    "本项目基于姿态估计和语音关键词分类模型打造了一款简单实用的人机交互新玩法。\n",
    "\n",
    "项目演示基于PyGame超级玛丽(PS: 有兴趣的小伙伴可以尝试其他好玩的游戏), 通过姿态估计模型提取几何太特征和运动特征翻译人体姿势指令，整个过程运动量还是比较大，很适合娱乐的同时减肥健身； 另一方面运动累了也可以切换到语音模式，让人机交互更接近真实感。\n",
    "\n",
    "基于本项目小伙伴还可以发挥更多的想象，比如练习外语，健身APP， 抑或是用PaddleGAN来点元宇宙的错觉，抑或是玩玩真机网友之类， 等等等等....\n",
    "\n",
    "**本项目的GitHub地址**: todo\n",
    "\n",
    "**注意: 两天参赛时间现撸代码，还存在很多瑕疵，所以本项目还在持续优化过程中，欢迎大家提出宝贵的意见，互相学习交流。**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**B站视频体验如下：**\n",
    "\n",
    "<div align = \"center\">\n",
    "<iframe style=\"width:50%;height: 450px;\" src=\"//player.bilibili.com/player.html?aid=977379091&bvid=BV1d44y1E7cz&cid=464965574&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n",
    "</div>\n",
    "\n",
    "**b站视频链接：[https://www.bilibili.com/video/BV1B64y1i7GM](https://www.bilibili.com/video/BV1B64y1i7GM)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 功能模块\n",
    "\n",
    "<div align = \"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/7470c39c04b04980960f581cf265716ba5a9a07fcb9b4230a7c4d1d98066875d\" alt=\"图片替换文本\" width=\"700\" height=\"600\" align=\"center\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 超级玛丽游戏\n",
    "\n",
    "一款载着满满儿时记忆的游戏, 在GitHub已有大佬基于PyGame已经完美复现， 作者已经实现到了第4关。\n",
    "\n",
    "GitHub地址: https://github.com/justinmeister/Mario-Level-1\n",
    "\n",
    "本项目对于交互部分做了少量的修改， 原项目是通过PyGame监听的按键操作，在本项目中将其他模块的指令放到队列中替代按键信号。\n",
    "\n",
    "<div align = \"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/0a1d04667ee64c1b8b58305ba83873a1af400ad44dfb4d01a1b3b52934fe23c3\" alt=\"图片替换文本\" width=\"500\" height=\"400\" align=\"center\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 人体关键点估计\n",
    "\n",
    "因人机交互对模型推理的高实时性要求，调研过多个模型之后， 最终选型采用的是PaddleDetection开源的PicoDet-S-Pedestrian以及PP-TinyPose， 模型推理时间单帧20ms左右，速度和效果都能满足要求。\n",
    "\n",
    "PP-TinyPose是PaddleDetecion针对移动端设备优化的实时姿态检测模型，可流畅地在移动端设备上执行多人姿态估计任务。借助PaddleDetecion自研的优秀轻量级检测模型PicoDet,我们同时提供了特色的轻量级垂类行人检测模型。\n",
    "\n",
    "**PP-TinyPose** 链接: https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.3/configs/keypoint/tiny_pose\n",
    "\n",
    "<div align = \"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/51dabfea3c844737bdc4d821bc390c7d853fc1e201744b999cb20aa1fb92e9d7\" alt=\"图片替换文本\" width=\"700\" height=\"600\" align=\"center\" />\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "考虑到额外的动作模型会增加指令的延迟，本项目只是将得到的关键点基于坐标信息进行简单的分类，基本也能满足需求。\n",
    "\n",
    "<div align = \"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/b90f3c6cecc842238cb3c7c518d683d1c4fc067cd0f84ef59e192ca506d4e948\" alt=\"图片替换文本\" width=\"700\" height=\"600\" align=\"center\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!git clone  PaddleDetection\n",
    "%cd PaddleDetection\n",
    "!python3 deploy/python/det_keypoint_unite_infer.py --det_model_dir=outut_inference/picodet_s_192_pedestrian --keypoint_model_dir=outut_inference/tinypose_128x96 --image_file=demo/000000014439.jpg --device=GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 语音分类训练\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**语音样本采集**\n",
    "\n",
    "目前AIStudio不支持在线采集，可以下载代码到本地运行:\n",
    "\n",
    "`!python speech_cmd_cls/generate_data.py`\n",
    "\n",
    "借助PyAudio第三方库, 上述语音采集脚本可自动录制声音，语音只需要采集游戏玩家7个关键字的声音，并以500ms间隔切割保存到对应目录，每个关键字大概录制2~3分钟就够了。时间充分的话，也可以按需扩充样本。\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**语音数据清洗**\n",
    "\n",
    "对于无声的、电流声的、或是听起来不清晰的录音片段，需要移动到第8个目录(名称: 其他)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**语音数据预处理**\n",
    "\n",
    "借助第三方库librosa, 加载音频文件，提取melspectrogram特征，并过滤掉一些低分贝音频帧。\n",
    "\n",
    "`!python speech_cmd_cls/preprocess.py`\n",
    "\n",
    "借助第三方库librosa, 加载音频文件，提取melspectrogram特征，并过滤掉一些低分贝音频帧。\n",
    "\n",
    "`!python speech_cmd_cls/preprocess.py`\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**ps: 文件夹下speech_cmd_cls/data是录制的作者的语音，方便大家测试。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/speech_cmd_cls\n",
      "标签名： ['左', '右', '下', '停', '跑', '跳', '打', '其它']\n",
      "preprocess data finished\n"
     ]
    }
   ],
   "source": [
    "#数据预处理\n",
    "%cd  speech_cmd_cls/\n",
    "!python preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechCommandModel(\n",
      "  (conv1): Conv2D(126, 10, kernel_size=[5, 1], padding=SAME, data_format=NCHW)\n",
      "  (relu1): ReLU()\n",
      "  (bn1): BatchNorm2D(num_features=10, momentum=0.9, epsilon=1e-05)\n",
      "  (conv2): Conv2D(10, 1, kernel_size=[5, 1], padding=SAME, data_format=NCHW)\n",
      "  (relu2): ReLU()\n",
      "  (bn2): BatchNorm2D(num_features=1, momentum=0.9, epsilon=1e-05)\n",
      "  (lstm1): LSTM(80, 64\n",
      "    (0): BiRNN(\n",
      "      (cell_fw): LSTMCell(80, 64)\n",
      "      (cell_bw): LSTMCell(80, 64)\n",
      "    )\n",
      "  )\n",
      "  (lstm2): LSTM(128, 64\n",
      "    (0): BiRNN(\n",
      "      (cell_fw): LSTMCell(128, 64)\n",
      "      (cell_bw): LSTMCell(128, 64)\n",
      "    )\n",
      "  )\n",
      "  (query): Linear(in_features=128, out_features=128, dtype=float32)\n",
      "  (softmax): Softmax(axis=-1)\n",
      "  (fc1): Linear(in_features=128, out_features=64, dtype=float32)\n",
      "  (fc1_relu): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=32, dtype=float32)\n",
      "  (classifier): Linear(in_features=32, out_features=8, dtype=float32)\n",
      "  (cls_softmax): Softmax(axis=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#简单搭建一个自定义带注意力的LSTM网络结构\n",
    "from paddle import nn\n",
    "class SpeechCommandModel(nn.Layer):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SpeechCommandModel, self).__init__()\n",
    "        self.conv1 =  nn.Conv2D(126, 10, (5, 1), padding=\"SAME\")\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2D(10)\n",
    "\n",
    "        self.conv2 =  nn.Conv2D(10, 1, (5, 1), padding=\"SAME\")\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2D(1)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=80, \n",
    "                                   hidden_size=64, \n",
    "                                   direction=\"bidirect\")\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=128, \n",
    "                                   hidden_size=64, \n",
    "                                   direction=\"bidirect\")\n",
    "\n",
    "        self.query = nn.Linear(128, 128)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc1_relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "        self.cls_softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = x.squeeze(axis=-1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x.squeeze(axis=1)\n",
    "\n",
    "        q = self.query(x)\n",
    "        attScores  = paddle.matmul(q, x, transpose_y=True)\n",
    "        attScores = self.softmax(attScores)\n",
    "        attVector = paddle.matmul(attScores, x)\n",
    "        # print(attVector.shape)\n",
    "\n",
    "        output = self.fc1(attVector)\n",
    "        output = self.fc1_relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.classifier(output)\n",
    "        output = self.cls_softmax(output)\n",
    "        print(output)\n",
    "    \n",
    "        return output\n",
    "\n",
    "model = SpeechCommandModel(num_classes = 8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**模型训练**\n",
    "\n",
    "使用飞桨的高层API对语音网络进行训练,  训练的准确率在95%左右\n",
    "\n",
    "即使没有GPU在飞桨框架下训练这个小网络也非常的快。\n",
    "\n",
    "`!python speech_cmd_cls/train.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/20\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:653: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9538 - 17ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.6995 - acc: 0.9657 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 2/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9551 - 16ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.5585 - acc: 0.9714 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 3/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9525 - 16ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.4175 - acc: 0.9771 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 4/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9564 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.5593 - acc: 0.9714 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 5/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9538 - 13ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.3246 - acc: 0.9714 - 5ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 6/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9447 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.5576 - acc: 0.9714 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 7/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9460 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.4488 - acc: 0.9714 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 8/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9525 - 15ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.7026 - acc: 0.9429 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 9/20\n",
      "step 193/193 [==============================] - loss: 1.7740 - acc: 0.9389 - 15ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.7024 - acc: 0.9486 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 10/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9460 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.5597 - acc: 0.9543 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 11/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9467 - 15ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.5596 - acc: 0.9657 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 12/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9506 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.5625 - acc: 0.9714 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 13/20\n",
      "step 193/193 [==============================] - loss: 1.7740 - acc: 0.9571 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.5593 - acc: 0.9657 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 14/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9525 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.6989 - acc: 0.9600 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 15/20\n",
      "step 193/193 [==============================] - loss: 1.7740 - acc: 0.9512 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.8454 - acc: 0.9543 - 6ms/step        \n",
      "Eval samples: 175\n",
      "Epoch 16/20\n",
      "step 193/193 [==============================] - loss: 1.7740 - acc: 0.9473 - 15ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.7026 - acc: 0.9543 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 17/20\n",
      "step 193/193 [==============================] - loss: 1.2741 - acc: 0.9519 - 15ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.3661 - acc: 0.9771 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 18/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9590 - 15ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.4335 - acc: 0.9714 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 19/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9590 - 14ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.6870 - acc: 0.9657 - 6ms/step         \n",
      "Eval samples: 175\n",
      "Epoch 20/20\n",
      "step 193/193 [==============================] - loss: 1.2740 - acc: 0.9545 - 15ms/step        \n",
      "Eval begin...\n",
      "step 22/22 [==============================] - loss: 1.6629 - acc: 0.9486 - 6ms/step         \n",
      "Eval samples: 175\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**模型评估和预测**\n",
    "\n",
    "训练完成可以对模型进行初步评估，也可以线下使用麦克风对模型效果进行实时验证\n",
    "\n",
    "`!python speech_cmd_cls/eval.py`\n",
    "\n",
    "`!python speech_cmd_cls/realtime_infer.py`\n",
    "\n",
    "**特别注意: 即使在验证集上训练出效果不错的模型，但是在这个小网络和小数据集上泛化能力相对较弱，当更换设备，更换说话人，或是更换到不同噪音背景的环境，效果可能会非常不理想。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "Eval begin...\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n",
      "step 3/3 - loss: 1.3763 - acc: 0.9543 - 27ms/step\n",
      "Eval samples: 175\n",
      "{'loss': [1.3763338], 'acc': 0.9542857142857143}\n"
     ]
    }
   ],
   "source": [
    "!python eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 下载完整代码在本地运行\n",
    "\n",
    "作者使用的环境是Ubuntu 18.04， conda环境， \n",
    "\n",
    "```\n",
    "git clone\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 写在最后\n",
    "\n",
    "这次能获得极客奖感到非常意外, 特别感谢百度飞桨举办的这次活动和百都科技杨总的大力支持，感谢宇澎现场的倾力付出，以及成都领航团和本次活动的小伙伴们，非常感谢大伙儿的投票支持！\n",
    "\n",
    "很荣幸得到参赛选手和评委们的极大鼓励，本项目将继续优化下去，希望能跟大家多多学习和交流，欢迎大伙儿提出批评意见和优化建议！\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 关于作者\n",
    "\n",
    "* 成都飞桨领航团团长\n",
    "* PPDE\n",
    "* AICA三期学员\n",
    "\n",
    "我在AI Studio上获得钻石等级，点亮10个徽章，来互关呀~ \n",
    "https://aistudio.baidu.com/aistudio/personalcenter/thirdview/89442"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
